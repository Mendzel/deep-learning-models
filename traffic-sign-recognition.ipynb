{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":191501,"sourceType":"datasetVersion","datasetId":82373}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Traffic Sign Recognition Model\n\n**Dataset**: [GTSRB - German Traffic Sign Recognition Benchmark](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign)\n\n**Goal**: To create as accurate model as possible with output model weighing less than 100MB so it's usabel on Raspberry Pi with compressing in TensorFlow Lite\n\n**outcome: 95.26% of accuracy on a test test and 77MB model**\n\n<hr>\nHere I've decided to use batch size of 16 with pictures of 32x32 pixels. \nI tried using bigger/smaller pictures with different batch sizes. While lowering batch size and increasing image size didn't affect the accuracy that much I've decided to stick to those values to decrease training time significantly.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Rescaling, Conv2D, Dense, MaxPooling2D, Dropout, Flatten\n\nPATH = '/kaggle/input/gtsrb-german-traffic-sign/'\nBATCH_SIZE = 16\nIMAGE_WIDTH = 32\nIMAGE_HEIGHT = 32","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-14T10:02:35.477476Z","iopub.execute_input":"2025-04-14T10:02:35.477729Z","iopub.status.idle":"2025-04-14T10:02:51.126888Z","shell.execute_reply.started":"2025-04-14T10:02:35.477704Z","shell.execute_reply":"2025-04-14T10:02:51.125842Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"After imports I've exctracted tabular data from Train.csv and extracted file paths and labels to create traning dataset. \nI looped over entire dataset and processed every image (basically loading and resizing every picture), then I shuffled dataset and created batches.\nAt the end dataset was split in two for train and validation datasets with ratio 8:2.","metadata":{}},{"cell_type":"code","source":"# Load file paths and labels\ndata = pd.read_csv(PATH + 'Train.csv')\nfile_names = data['Path'].values\nfile_paths = tf.constant([os.path.join(PATH, fn) for fn in file_names])\n\nlabels = data['ClassId'].values\nlabels = tf.constant(labels)\nnum_classes = len(np.unique(labels))\n\n# Convert to TensorFlow Dataset and normalize images\ndef process_image(path, label):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.image.resize(img, [IMAGE_WIDTH, IMAGE_HEIGHT])\n    return img, label\n\ndataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\ndataset = dataset.map(process_image)\ndataset = dataset.shuffle(buffer_size=len(file_paths), seed=42)\ndataset = dataset.batch(BATCH_SIZE)\ntrain, valid = tf.keras.utils.split_dataset(dataset, left_size=0.8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:04:17.261451Z","iopub.execute_input":"2025-04-14T10:04:17.261894Z","iopub.status.idle":"2025-04-14T10:06:19.329016Z","shell.execute_reply.started":"2025-04-14T10:04:17.261857Z","shell.execute_reply":"2025-04-14T10:06:19.327882Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Now I am loading test set, for that we need to extract labels from Test.csv and then we provide image directory to image_dataset_from_directory function. <br><br>\n<i>It would have been great to use the same function for prodiving test and validation sets, but TensorFlow couldn't infer labels correctly even while the folder structure seemed correct. So I've decided to load dataset manually (almost) with from_tensor_slices.</i>","metadata":{}},{"cell_type":"code","source":"test_labels = list(pd.read_csv(PATH + 'Test.csv', usecols=['ClassId']).to_numpy().flatten())\ntest = tf.keras.preprocessing.image_dataset_from_directory(\n    PATH + 'Test',\n    labels=test_labels,\n    color_mode='rgb',\n    batch_size=BATCH_SIZE,\n    image_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n    interpolation='bilinear',\n    crop_to_aspect_ratio=True,\n    verbose=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:06:19.330540Z","iopub.execute_input":"2025-04-14T10:06:19.330840Z","iopub.status.idle":"2025-04-14T10:07:45.636617Z","shell.execute_reply.started":"2025-04-14T10:06:19.330815Z","shell.execute_reply":"2025-04-14T10:07:45.635570Z"}},"outputs":[{"name":"stdout","text":"Found 12630 files belonging to 43 classes.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Here I am preprocessing the data with optimizing training time by prefetching data into buffers. Also I am augmenting images in the training set.","metadata":{}},{"cell_type":"code","source":"data_augmentation = Sequential([\n    tf.keras.layers.RandomRotation(0.04),\n    tf.keras.layers.RandomZoom(0.1),\n    tf.keras.layers.RandomTranslation(0.1, 0.1),\n    tf.keras.layers.RandomBrightness(factor=0.15),\n    tf.keras.layers.GaussianNoise(0.12),\n])\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef prepare(ds, shuffle=False, augment=False):\n  # Use data augmentation only on the training set.\n  if augment:\n    ds = ds.map(lambda x, y: (data_augmentation(x), y), \n                num_parallel_calls=AUTOTUNE)\n\n  # Use buffered prefetching on all datasets.\n  return ds.prefetch(buffer_size=AUTOTUNE)\n\ntrain_ds = prepare(train, augment=True)\nval_ds = prepare(valid)\ntest_ds = prepare(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:07:45.638156Z","iopub.execute_input":"2025-04-14T10:07:45.638450Z","iopub.status.idle":"2025-04-14T10:07:45.932563Z","shell.execute_reply.started":"2025-04-14T10:07:45.638425Z","shell.execute_reply":"2025-04-14T10:07:45.931589Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Displaying batch of images.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30, 30))\nfor x, y in train_ds.take(1):\n  for i in range(BATCH_SIZE):\n    ax = plt.subplot(4, 4, i + 1)\n    # Get the i-th label\n    label = y[i]\n\n    # Convert to displayable uint8 image\n    display_image = (x[i]).numpy().astype(\"uint8\")\n\n    # Show image\n    plt.imshow(display_image)\n    plt.title(label.numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:59:36.671670Z","iopub.execute_input":"2025-04-09T18:59:36.671995Z","iopub.status.idle":"2025-04-09T18:59:40.353847Z","shell.execute_reply.started":"2025-04-09T18:59:36.671969Z","shell.execute_reply":"2025-04-09T18:59:40.352975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So here is the heart of my model, I've decided on certain kernel_sizes based on Yan Han and Erdal Oruklu paper:\n[Traffic sign recognition based on the NVIDIA Jetson TX1 embedded system using convolutional neural networks](https://www.researchgate.net/publication/320606581_Traffic_sign_recognition_based_on_the_NVIDIA_Jetson_TX1_embedded_system_using_convolutional_neural_networks)\n<hr>\nTo avoid a dead relu problem, I've used LeakyRelu activation for convolutions, while sticking to basic relu in Dense layers. First tries were based on around 25M parameters with higher filters and conv layers number. Final version uses around 6.4M parameters and is much lighter, I probably could have shrinked it even more, but decrease in weight to 77MB satisfied my needs without hurting accuracy and models ability to generalize unseen data.","metadata":{}},{"cell_type":"code","source":"def create_model():\n    model = Sequential([\n        tf.keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)),\n        # Data normalization\n        Rescaling(1./255),\n        # Convolutional layers\n        Conv2D(50, kernel_size=(9,9), strides=1, padding=\"same\"),\n        tf.keras.layers.LeakyReLU(),\n        MaxPooling2D(pool_size=(2, 2),strides=(1, 1), padding=\"same\"),\n        Conv2D(70, kernel_size=(7,7), strides=1, padding=\"same\"),\n        tf.keras.layers.LeakyReLU(),\n        MaxPooling2D(pool_size=(2, 2),strides=(1, 1), padding=\"same\"),\n        Conv2D(100, kernel_size=(3,3), strides=1, padding=\"same\"),\n        tf.keras.layers.LeakyReLU(),\n        # Dense layers\n        Flatten(),\n        Dense(60, activation='relu'),\n        Dropout(0.2),\n        Dense(60, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:07:52.371789Z","iopub.execute_input":"2025-04-14T10:07:52.372141Z","iopub.status.idle":"2025-04-14T10:07:52.378841Z","shell.execute_reply.started":"2025-04-14T10:07:52.372113Z","shell.execute_reply":"2025-04-14T10:07:52.377781Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Here I am specyfing my early stopper so I dont have to worry about number of epochs ran to train the model. Instead I watch the accuracy from validation set to rise at least by 0.5% within 20 epochs.","metadata":{}},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience=20, restore_best_weights=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:01:30.669845Z","iopub.execute_input":"2025-04-09T19:01:30.670129Z","iopub.status.idle":"2025-04-09T19:01:30.673865Z","shell.execute_reply.started":"2025-04-09T19:01:30.670110Z","shell.execute_reply":"2025-04-09T19:01:30.672983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:07:55.349038Z","iopub.execute_input":"2025-04-14T10:07:55.349378Z","iopub.status.idle":"2025-04-14T10:07:55.502002Z","shell.execute_reply.started":"2025-04-14T10:07:55.349352Z","shell.execute_reply":"2025-04-14T10:07:55.501033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │          \u001b[38;5;34m12,200\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m70\u001b[0m)          │         \u001b[38;5;34m171,570\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m70\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m70\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │          \u001b[38;5;34m63,100\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m102400\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)                  │       \u001b[38;5;34m6,144,060\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)                  │           \u001b[38;5;34m3,660\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m)                  │           \u001b[38;5;34m2,623\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,200</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">171,570</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">63,100</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">102400</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144,060</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,660</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,623</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,397,213\u001b[0m (24.40 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,397,213</span> (24.40 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,397,213\u001b[0m (24.40 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,397,213</span> (24.40 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"And now we start to train the model, number of epochs doesnt really matter as long as its a high number, early stopping is going to take care of ending the training when its suitable to do so. All hyperparameters in here were chosen based on some trial and errors.","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer=Adam(learning_rate=0.0003),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])\n\nhistory = model.fit(train_ds, epochs=1000, verbose=True, callbacks=[early_stop],\n                    validation_data=val_ds, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:01:42.211794Z","iopub.execute_input":"2025-04-09T19:01:42.212118Z","iopub.status.idle":"2025-04-09T19:14:35.484945Z","shell.execute_reply.started":"2025-04-09T19:01:42.212091Z","shell.execute_reply":"2025-04-09T19:14:35.484134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plotting our efficiency on the training and validation data.","metadata":{}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(100)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:14:40.248742Z","iopub.execute_input":"2025-04-09T19:14:40.249034Z","iopub.status.idle":"2025-04-09T19:14:40.590590Z","shell.execute_reply.started":"2025-04-09T19:14:40.249012Z","shell.execute_reply":"2025-04-09T19:14:40.589711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Saving keras model.","metadata":{}},{"cell_type":"code","source":"model.save('tsr_maxpool_v3.keras')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:14:55.893088Z","iopub.execute_input":"2025-04-09T19:14:55.893435Z","iopub.status.idle":"2025-04-09T19:14:56.204098Z","shell.execute_reply.started":"2025-04-09T19:14:55.893397Z","shell.execute_reply":"2025-04-09T19:14:56.203375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluating model on the test set.","metadata":{}},{"cell_type":"code","source":"results = model.evaluate(test, batch_size=BATCH_SIZE)\nprint(\"test loss, test acc:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:14:58.904344Z","iopub.execute_input":"2025-04-09T19:14:58.904735Z","iopub.status.idle":"2025-04-09T19:15:08.526017Z","shell.execute_reply.started":"2025-04-09T19:14:58.904703Z","shell.execute_reply":"2025-04-09T19:15:08.525096Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualizing predicted and original labels for batches of test data.","metadata":{}},{"cell_type":"code","source":"for images, labels in test.take(1):  # This grabs a batch\n    plt.figure(figsize=(20, 20))\n    for i in range(BATCH_SIZE):  # Display 16 images\n        ax = plt.subplot(4, 4, i + 1)\n\n        image = images[i]\n        label = labels[i]\n\n        predict_image = tf.expand_dims(image, axis=0)\n        prediction = model.predict(predict_image, verbose=0)\n        y_hat = np.argmax(prediction, axis=1)[0]\n\n        plt.imshow(image.numpy().astype(\"uint8\"))\n        plt.title(f'Original: {label.numpy()}, Predicted: {y_hat}')\n        plt.axis(\"off\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:15:33.484290Z","iopub.execute_input":"2025-04-09T19:15:33.484641Z","iopub.status.idle":"2025-04-09T19:15:36.745033Z","shell.execute_reply.started":"2025-04-09T19:15:33.484611Z","shell.execute_reply":"2025-04-09T19:15:36.744143Z"}},"outputs":[],"execution_count":null}]}